{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems that a Particle filter is good for\n",
    "\n",
    "**multimodal**: tracking more than one object simultaneously\n",
    "**occlusions**: an object hiding behind another\n",
    "**nonlinear behavior**: in both measurements and system\n",
    "**non-gaussian noise**: in computer vison the algorithm can mistake a part of the background for part of the object\n",
    "**continous**: can use continous functions\n",
    "**multivariate**: can track several attributes of the state\n",
    "**unknown process model**: can track an unknown process model\n",
    "\n",
    "**Where these other methods fail**:\n",
    "\n",
    "**Discrete bayes filter**: has most of these attributes but is discrete and univariate\n",
    "\n",
    "**Kalman**: Only works for unimodal, linear systems with gaussian noise\n",
    "\n",
    "**UKF**: Not multimodal, and can't handle occlusions. Good for fairly non-gaussian noise but not good when its very non-gaussian\n",
    "\n",
    "**EKF**: Same as UKF, just more sensitive to strong linearities and non-gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Particle Filter Algorithm\n",
    "\n",
    "1. **Randomly Generate a bunch of particles**\n",
    "These particles can have whatever state variables you need to measure\n",
    "\n",
    "2. **Predict the next state of the particles**\n",
    "Move particles based on how you predict the real system is behaving\n",
    "\n",
    "3. **Update**\n",
    "Update the weighing of the particles based on the measurement. particles that closely match the measurements are weighted higher than the particles which dont match the measurements very well.\n",
    "\n",
    "4. **Resample**\n",
    "Discard highly improbably particles and replace them with copies of the more probable particles.\n",
    "\n",
    "5. **Compute Estimate**\n",
    "Can compute a weighted mean and covariance of the set of particles to get a state estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideally the weight of each particle would be the probability that it represents the true position of the vehicle. The probability is rarely computable but we only require that the weights are proportional to the probability.\n",
    "\n",
    "**Predict Step**:\n",
    "Usually we can use the commands that are sent to the vehicle, or some equation describing the movement that is being observed. The thing to note is that these commands and equations need to have a bit of simulated noise in order to give the particles a chance to notice any non-linear behavior\n",
    "\n",
    "**Update Step**: The update step implements Bayes Theorem. So assign a probability to each position called the $prior$, when a new measurement comes in we multiply the current probability of that position (prior) by the $likelihood$ that the measurement matched that location\n",
    "\n",
    "$$\\begin{aligned}P(x \\mid z) &= \\frac{P(z \\mid x)\\, P(x)}{P(z)} \\\\\n",
    " &= \\frac{\\mathtt{likelihood}\\times \\mathtt{prior}}{\\mathtt{normalization}}\\end{aligned}$$\n",
    "\n",
    "This part of the algorithm is called the Sequesntial Importance Sampling. The equations for the weights is called the importance density. The weights are the likelihood in bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degeneracy Problem\n",
    "\n",
    "The algorithm above SIS (Sequential Importance Sampling), suffers from a $degeneracy problem$. Which basically means that over time as the filter runs, any particle that is not near the measurements will acquire an extremely low weight, meaning that the system will be wasting memory on thousands of samples that do not contribute towards a better estimate of the thing we are trying to track.\n",
    "\n",
    "To deal with these we use a resampling algorithm. Algorithms dump any particles with a certain low weight then replace them with copies of particles with higher weight. These particles are  slightly different from the particles they copied from because the predict step injects some noise.\n",
    "\n",
    "**determining when to resample**: Don't resample every epoch but rather, we resample by using the $effective N$ \n",
    "\n",
    "$$\\hat{N}_\\text{eff} = \\frac{1}{\\sum w^2}$$\n",
    "\n",
    "So if the effective N falls below some threshhold we call the resampling algorithm. Usuall N/2, where N is the number of particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of sensor error on the particle filter\n",
    "\n",
    "Opposed to the Kalman filter, the particle filter has the posibility of performing poorly if the standard deviation in the measurements is very small. This is due to how the particles are generated, if the standard deviation is 0.1, a robot is at position (1,1) and a particle is generated at (2,2) then the particle is 14 standard deviations away giving it a very small probability of being selected. So a very good sensor could actually hurt the estimate.\n",
    "\n",
    "**some fixes to this problem**: \n",
    "    - artificially increase the sensor noise std deviation so the particle filter algorithm will accept more points as matching the robots probability distribuition. But this is not optimal because youre creating a bunch of points that are not a good match.\n",
    "    \n",
    "    - use more samples. this works great, but comes at the expense of using a lot of memory.\n",
    "    \n",
    "    - Best thing is knowing the vehicles starting position, it gives the filter an easier time. But don't be too exact because if the particles are not disperced enough the filter might not capture the non-linearities of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "\n",
    "So it might be hard to actually figure out the probability distribuition that describes the position and movement of a robot to compute an integral using monte carlo methods. If our state model is incorrect we can still weight the samples based off the probability distribuition that we interested in. This is effectively what happens when you weight the samples based off the measurement information. So our state model can be a bit off, because the measurement information will resemble the actual probablily distribuition of what we're trying to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
